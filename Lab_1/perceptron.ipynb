{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Creation and Analysis\n",
    "\n",
    "In this section, we are going to create and analyze data for mail spam detection.\n",
    "\n",
    "Mathematically, the data is given by the set:\n",
    "\n",
    "$$\n",
    "X = \\{ (x_1, y_1), (x_2, y_2), \\dots, (x_i, y_i) \\}\n",
    "$$\n",
    "\n",
    "where $x_i \\in \\mathbb{R}^D$ and $D$ is the dimensionality of the feature vector. In the mail spam detection task, we will set $D = 2$.\n",
    "\n",
    "We will generate two datasets: one that is perfectly linearly separable and one that is not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a Linearly Separable Dataset\n",
    "\n",
    "We are going to create the first dataset using the `numpy` library. \n",
    "Remember that the dataset needs to be represented as a tuple of two numpy arrays:\n",
    "- `X`: A numpy array of shape `(num_samples, 2)` representing the feature vectors.\n",
    "- `y`: A numpy array of shape `(num_samples,)` representing the target labels.\n",
    "\n",
    "We will create the dataset by generating normally distributed data with the help of `np.random.randn`, which returns samples from a normal distribution. To ensure the data is linearly separable, we will shift these distributions with an offset so they do not overlap. We can also control the spread of the data by adjusting the standard deviations (sigma values). \n",
    "\n",
    "For this email spam detection problem:\n",
    "- The **negative class** (non-spam) will have greater dispersion, representing the variable nature of regular emails.\n",
    "- The **spam class** will have smaller variation and be more tightly clustered.\n",
    "\n",
    "#### Task:\n",
    "In the function `create_data`, after generating the four numpy arrays:\n",
    "1. **Concatenate the arrays**: \n",
    "    - Concatenate `X_neg` and `X_pos` to form the final `X` numpy array of shape `(X_neg.shape[0] + X_pos.shape[0], 2)`.\n",
    "    - Concatenate `y_neg` and `y_pos` to form the final `y` numpy array of shape `(y_neg.shape[0] + y_pos.shape[0], )`.\n",
    "    - *Hint*: You can use `np.vstack()` to vertically stack the feature arrays and `np.hstack()` to stack the labels.\n",
    "\n",
    "2. **Randomly permute the data**: \n",
    "    - Randomly shuffle the data to avoid having 200 positive examples followed by 200 negative examples, as some algorithms (like the Perceptron) are sensitive to this issue.\n",
    "    - *Hint*: You can use `np.random.permutation` to shuffle the data indices and reorder both `X` and `y` accordingly.\n",
    "\n",
    "3. **Bias trick**:\n",
    "    - we are going to implement the algorithm using the bias trick in order for the dimensions to be comatible we need to add one extra dimension to $x_i$\n",
    "    - *Hint* \n",
    "3. **Return the final `X` and `y` arrays**: \n",
    "    - Make sure to return the shuffled `X` and `y` arrays as the final output of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(num_samples_positive=200, num_samples_negative=200, offset_pos=np.full(2, 2), offset_neg=np.full(1, 1), sigma_pos=0.5, sigma_neg=0.75):\n",
    "\n",
    "    X_neg = sigma_pos * np.random.randn(num_samples_negative, 2) - offset_neg\n",
    "    y_neg = -np.ones((num_samples_negative, 1))\n",
    "\n",
    "    X_pos = sigma_neg * np.random.randn(num_samples_positive, 2) + offset_pos \n",
    "    y_pos = np.ones((num_samples_positive, 1))\n",
    "\n",
    "    X, y = None, None\n",
    "\n",
    "    ###### Your code here ######\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_positive = 200\n",
    "num_samples_negative = 200\n",
    "\n",
    "# create the linearly separable dataset\n",
    "X, y = create_data(num_samples_positive, num_samples_negative)\n",
    "\n",
    "#create the non-linearly separable dataset\n",
    "X_n, y_n = create_data(num_samples_positive, num_samples_negative, sigma_pos=1.2, sigma_neg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, title='Linearly Separable Dataset'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[y == -1][:, 0], X[y == -1][:, 1], color='red', label='Not spam : -1')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Spam : 1')\n",
    "    plt.title(f'{title} with {num_samples_negative} negative and {num_samples_positive} positive samples')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_data(X, y)\n",
    "plot_data(X_n, y_n, title='Non-Linearly Separable Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PLA Implementation\n",
    "\n",
    "In this section, we are going to implement two main components of the Perceptron Learning Algorithm (PLA):\n",
    "1. **Prediction Function**: This function will return the predicted class of a given example based on the current weight vector and the PLA hypothesis function.\n",
    "2. **Weight Update Function**: This function will return the updated weight vector based on the current training example and its corresponding label.\n",
    "\n",
    "### 2.1 Prediction Function\n",
    "\n",
    "The `predict` function should return the predicted class of the given input example based on the current weight vector and the PLA hypothesis. The hypothesis predicts the sign of the weighted sum of the features.\n",
    "\n",
    "#### Inputs:\n",
    "- `x`: The input example (a feature vector).\n",
    "- `w`: The current weight vector.\n",
    "\n",
    "#### Variables:\n",
    "- `pred`: The predicted class label.\n",
    "\n",
    "#### Return:\n",
    "- `pred`: The predicted class (-1 or 1).\n",
    "\n",
    "### 2.2 Weight Update Function\n",
    "\n",
    "The `update_weights` function should return the updated weight vector after adjusting the current weight based on the misclassified training example and its true label.\n",
    "\n",
    "#### Inputs:\n",
    "- `x_t`: The input feature vector at timestep `t`.\n",
    "- `y_t`: The true label corresponding to `x_t`.\n",
    "- `w_t`: The weight vector at timestep `t`.\n",
    "\n",
    "#### Variables:\n",
    "- `w_t_1`: The updated weight vector after applying the update rule.\n",
    "\n",
    "#### Return:\n",
    "- `w_t_1`: The updated weight vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "\n",
    "    pred = None\n",
    "\n",
    "    ######### Your code here #########\n",
    "\n",
    "\n",
    "    #################################\n",
    "\n",
    "    return pred\n",
    "\n",
    "def update_weights(x_t, y_t, w_t):\n",
    "\n",
    "    w_t_1 = None\n",
    "\n",
    "    ######### Your code here #########\n",
    "\n",
    "\n",
    "    #################################\n",
    "\n",
    "    return w_t_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Algorithm Training and Testing\n",
    "\n",
    "In this section, we will focus on training and testing the perceptron algorithm. We will measure the algorithm's performance by checking how well it classifies emails as spam or not, based on the test set.\n",
    "\n",
    "According to Tom Mitchell's definition, we need to evaluate the performance \\( P \\) of the algorithm. In this case, we can see the perfermence as the number of misclassified examples.\n",
    "\n",
    "### 3.1 Compute Number of Misclassified Points\n",
    "\n",
    "We need to implement a function `compute_misclassified_points` that calculates the number of misclassified points in a given set. You should use vectorization to compute this efficiently without using loops.\n",
    "\n",
    "#### Input:\n",
    "- `X`: A set of points to classify (shape: `(no_points, 3)`).\n",
    "- `y`: The corresponding target labels (shape: `(no_points,)`).\n",
    "- `w`: The weight vector of the algorithm.\n",
    "\n",
    "#### Variables:\n",
    "- `misclassified_points`: The number of points that are misclassified.\n",
    "\n",
    "#### Return:\n",
    "- `misclassified_points`: The total number of misclassified points.\n",
    "\n",
    "### 3.2 Train the Perceptron\n",
    "\n",
    "The function `train_perceptron` is responsible for training the algorithm by optimizing the weight parameters \\( w \\). The algorithm will continue updating the weights while misclassified points exist, or until it reaches a specified iteration limit. For each update, the tuple `(w_t, w_t_1, x_t, y_t)` should be appended to the `save_for_visualization` list.\n",
    "\n",
    "#### Input:\n",
    "- `X_train`: The training input set (shape: `(no_points, 3)`).\n",
    "- `Y_train`: The corresponding training labels (shape: `(no_points,)`).\n",
    "- `total_iterations`: The number of iterations after which the algorithm should stop, if it doesn't converge.\n",
    "\n",
    "#### Variables:\n",
    "- `current_iteration`: Keeps track of the current iteration count.\n",
    "- `save_for_visualization`: A list of tuples containing `(w_t, w_t_1, x_t, y_t)` for each update to visualize decision boundaries.\n",
    "- `w_t`: The weight vector at time \\( t \\).\n",
    "- `w_t_1`: The weight vector at time \\( t + 1 \\).\n",
    "\n",
    "### 3.3 Test the Perceptron\n",
    "\n",
    "To evaluate the performance of the perceptron, we will compute the number of misclassified examples in the test set. The function `test_perceptron` should return the number of misclassified points from the test set.\n",
    "\n",
    "#### Input:\n",
    "- `X_test`: The set of test points.\n",
    "- `y_test`: The corresponding test labels.\n",
    "- `w`: The weights of the model.\n",
    "\n",
    "#### Variables:\n",
    "- `metric`: The number of misclassified points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_missclassified_points(X, y, w):\n",
    "    \n",
    "    missclassified_points = None\n",
    "\n",
    "    ######### Your code here #########\n",
    "    \n",
    "\n",
    "    #################################\n",
    "\n",
    "    return missclassified_points\n",
    "\n",
    "\n",
    "def train(X_train, y_train, total_iterations=1000):\n",
    "    \n",
    "    current_iteration = 0    \n",
    "    save_for_vizualization = []\n",
    "    w_t = np.ones(3)\n",
    "    w_t_1 = None\n",
    "\n",
    "    while compute_missclassified_points(X_train, y_train, w_t) != 0 and current_iteration < total_iterations:\n",
    "\n",
    "        ###### Your code here ######      \n",
    "      \n",
    "    \n",
    "\n",
    "        ########################\n",
    "\n",
    "    return w_t, save_for_vizualization\n",
    "\n",
    "def test(X_test, y_test, w):\n",
    "\n",
    "    metric = None\n",
    "\n",
    "    ######### Your code here #########\n",
    "\n",
    "\n",
    "    #################################\n",
    "\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X_train, y_train, X_test, y_test, w, title='Perceptron Algorithm'):\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_train[y_train == -1][:, 0], X_train[y_train == -1][:, 1], color='red', label='Class -1, train')\n",
    "    plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], color='blue', label='Class 1, train')\n",
    "    plt.scatter(X_test[y_test == -1][:, 0], X_test[y_test == -1][:, 1], color='red', alpha=0.5, label='Class -1, test')\n",
    "    plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='blue', alpha=0., label='Class 1, test')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    x1 = np.linspace(-2, 2, 100)\n",
    "    x2 = -(w[0] * x1 + w[2]) / w[1]\n",
    "    plt.plot(x1, x2, color='black', label='Decision Boundary')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train, Test, and Visualization Script\n",
    "\n",
    "Now that we have all the necessary building blocks, it's time to train and test the perceptron. First, we will create the train and test datasets. A common rule of thumb is to store 80% of the data in the training set and 20% in the test set.\n",
    "\n",
    "#### Steps:\n",
    "1. Split the dataset into training (80%) and test (20%) sets.\n",
    "2. Train the perceptron using the training set.\n",
    "4. Return the weights and the `save_for_visualization` list for further analysis.\n",
    "5. Test and print the number of misclassified exmaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "###### Your code here ######\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "# Train the perceptron algorithm\n",
    "\n",
    "# Test the perceptron algorithm and print the numnber of missclassified points\n",
    "\n",
    "###########################\n",
    "\n",
    "plot_decision_boundary(X_train, y_train, X_test, y_test, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Train, Test, and Visualization Script non-liear data\n",
    "\n",
    "Redo all the steps from section 3.4 but this time with the non linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "###### Your code here ######\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "# Train the perceptron algorithm\n",
    "\n",
    "# Test the perceptron algorithm and print the numnber of missclassified points\n",
    "\n",
    "###########################\n",
    "\n",
    "plot_decision_boundary(X_train, y_train, X_test, y_test, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Geometrical intuition\n",
    "\n",
    "The script will generate plots for each iteration to illustrate how the algorithm works with the geometrical intuition discussed in the lab.\n",
    "\n",
    "- The **orange line** represents the previous decision boundary.\n",
    "- The **orange arrow** represents the weight vector $\\mathbf{w} $, which is perpendicular to the decision boundary.\n",
    "- The **green dot** marks the misclassified example, and the **green arrow** represents the vector $ y(t) \\mathbf{x}(t) $.\n",
    "- The **black arrow** shows the newly computed weight vector $ \\mathbf{w} $, and the **black line** represents the corresponding updated decision boundary.\n",
    "\n",
    "Note: change the `folder_path` variable to the desired one to save the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_update_video(saved_for_vizualization, X_train, y_train, X_test, y_test, folder_path):\n",
    "    # Check if the folder exists, if not, create it\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    for i in range(len(saved_for_vizualization)):\n",
    "        w_t, w_t_1, x_t, y_t = saved_for_vizualization[i]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "        # Plot the data points\n",
    "        ax.scatter(X_train[y_train == -1][:, 0], X_train[y_train == -1][:, 1], color='red', label='Class -1')\n",
    "        ax.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], color='blue', label='Class 1')\n",
    "        ax.scatter(X_test[y_test == -1][:, 0], X_test[y_test == -1][:, 1], color='red', alpha=0.5, label='Class -1, test')\n",
    "        ax.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='blue', alpha=0.5, label='Class 1, test')\n",
    "\n",
    "        ax.set_title('Perceptron Algorithm')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Ensure the aspect ratio is equal to avoid distortion\n",
    "        ax.set_aspect('equal')  # Set aspect ratio to 1:1\n",
    "\n",
    "        # Set the limits of the plot to be between -5 and 5 on both axes\n",
    "        ax.set_xlim([-5, 5])\n",
    "        ax.set_ylim([-5, 5])\n",
    "\n",
    "        # Initial decision boundary (w_t)\n",
    "        x1 = np.linspace(-5, 5, 100)\n",
    "        y1 = -(w_t[0] * x1 + w_t[2]) / w_t[1]\n",
    "        ax.plot(x1, y1, color='orange', label='Initial decision Boundary')\n",
    "\n",
    "        # Updated decision boundary (w_t_1)\n",
    "        y2 = -(w_t_1[0] * x1 + w_t_1[2]) / w_t_1[1]\n",
    "        ax.plot(x1, y2, color='black', label='Updated decision Boundary')\n",
    "\n",
    "        # Highlight the point x_t being updated\n",
    "        ax.scatter(x_t[0], x_t[1], color='green')\n",
    "\n",
    "        ### Draw the normal vector (w_t) to the decision boundary in orange ###\n",
    "        ax.arrow(0, 0, w_t[0], w_t[1], \n",
    "                 head_width=0.1, head_length=0.1, fc='orange', ec='orange', color='orange', \n",
    "                 label='Normal Vector')\n",
    "\n",
    "        # Define the red vector based on the class label y_t\n",
    "        if y_t == -1:\n",
    "            red_vector = [-x_t[0], -x_t[1]]  # Opposite of x_t\n",
    "            ax.arrow(0, 0, red_vector[0], red_vector[1],\n",
    "                     head_width=0.1, head_length=0.1, fc='green', ec='green', color='green', \n",
    "                     label='Opposite x_t')\n",
    "        else:\n",
    "            red_vector = [x_t[0], x_t[1]]  # x_t vector\n",
    "            ax.arrow(0, 0, red_vector[0], red_vector[1],\n",
    "                     head_width=0.1, head_length=0.1, fc='green', ec='green', color='green', \n",
    "                     label='x_t')\n",
    "\n",
    "        # Calculate the resulting vector (sum of the red vector and orange vector)\n",
    "        result_vector_x = w_t[0] + red_vector[0]\n",
    "        result_vector_y = w_t[1] + red_vector[1]\n",
    "\n",
    "        # Draw the resulting vector as a black arrow\n",
    "        ax.arrow(0, 0, result_vector_x, result_vector_y,\n",
    "                 head_width=0.1, head_length=0.1, fc='black', ec='black', color='black', \n",
    "                 label='Resulting Vector (w_t + x_t)')\n",
    "\n",
    "        # Save the figure to the specified folder\n",
    "        plt.savefig(os.path.join(folder_path, f'plot_{i}.png'))\n",
    "\n",
    "        # Close the plot to avoid displaying it\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Image\n",
    "from ipywidgets import Button, HBox, VBox, Output\n",
    "\n",
    "folder_path = '/Users/mihnea.lumen/Desktop/UTCN/Teaching/ML_1/Solved/perceptron_images'\n",
    "create_update_video(saved_for_vizualization, X_train, y_train.reshape(-1), X_test, y_test.reshape(-1), folder_path)\n",
    "\n",
    "# Load image file names from the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n",
    "image_files.sort()  # Sort the files alphabetically\n",
    "\n",
    "# Display the first image\n",
    "current_image_index = 0\n",
    "\n",
    "# Create the output widget to display images\n",
    "output = Output()\n",
    "\n",
    "def show_image(index):\n",
    "    \"\"\"Display the image at the given index.\"\"\"\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        image_path = os.path.join(folder_path, image_files[index])\n",
    "        display(Image(filename=image_path))\n",
    "        \n",
    "# Define button click event handlers\n",
    "def on_next_button_clicked(b):\n",
    "    global current_image_index\n",
    "    if current_image_index < len(image_files) - 1:\n",
    "        current_image_index += 1\n",
    "        show_image(current_image_index)\n",
    "\n",
    "def on_prev_button_clicked(b):\n",
    "    global current_image_index\n",
    "    if current_image_index > 0:\n",
    "        current_image_index -= 1\n",
    "        show_image(current_image_index)\n",
    "\n",
    "# Create navigation buttons\n",
    "next_button = Button(description=\"Next\")\n",
    "prev_button = Button(description=\"Previous\")\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n",
    "\n",
    "# Display the initial image\n",
    "show_image(current_image_index)\n",
    "\n",
    "# Arrange the buttons and output in a layout\n",
    "display(VBox([HBox([prev_button, next_button]), output]))\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
