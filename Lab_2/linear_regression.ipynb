{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "The first task is to load and process the dataset. We have loaded the dataset for you. `X` is the design matrix with a shape of `(20640, 8)`, and the target vector `y` has a shape of `(20640,)`. The number of rows represents the number of examples in the dataset, and the number of columns in `X` represents the number of features per example. An entire row of `X` is called a **feature vector**.\n",
    "\n",
    "### 1.1 Load the California Housing Dataset\n",
    "\n",
    "We will use the `fetch_california_housing` dataset from `sklearn`. The cell below loads the dataset and displays the feature names.\n",
    "\n",
    "### 1.2 Data Preprocessing\n",
    "\n",
    "To evaluate the performance of our linear regression model, we need to split the data into a **training set** and a **test set**. We will:\n",
    "- Learn the model on the training set.\n",
    "- Test the model on the test set.\n",
    "\n",
    "#### Bias Trick:\n",
    "As in the PLA algorithm, we will take advantage of the **bias trick** by adding a column of ones to the feature matrix.\n",
    "\n",
    "#### Normalization:\n",
    "Since the features in the dataset have different ranges, we need to normalize the data. The normalization is performed using the following formula:\n",
    "\n",
    "$$\n",
    "x = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu $ is the mean of each feature.\n",
    "- $ \\sigma $ is the standard deviation of each feature.\n",
    "\n",
    "**Note:** The normalization parameters $ \\mu $ and $ \\sigma $ are computed **only from the training set** and then used to normalize both the training and test sets.\n",
    "\n",
    "#### Steps:\n",
    "- Split the dataset into training and test sets.\n",
    "- Normalize the data based on the training set.\n",
    "- Add a bias term (a column of ones) to the design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load boston housing price dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "print(f'The dataset has {X.shape[0]} samples and {X.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispalay the min max value of each feature\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{feature_names[i]}: min={np.min(X[:,i])}, max={np.max(X[:,i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #Output: X: a numpy array of shape (N, D+1), where a column of ones is concatenated to the input array X\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "    \n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return X\n",
    "\n",
    "def normalize(X, mean=None, std=None):\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        mean: a numpy array of shape (D,) containing the mean of each feature\n",
    "    #        std: a numpy array of shape (D,) containing the standard deviation of each feature\n",
    "    # Output: X: a numpy array of shape (N, D), where each feature is normalized by subtracting the mean and dividing by the standard deviation\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return X\n",
    "\n",
    "def split_data(X, y, ratio=0.8):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        ratio: a float number between 0 and 1, representing the ratio of training data\n",
    "    # Output: X_train: a numpy array of shape (N_train, D), containing the training data\n",
    "    #         y_train: a numpy array of shape (N_train,), containing the target for each training sample\n",
    "    #         X_test: a numpy array of shape (N_test, D), containing the testing data\n",
    "    #         y_test: a numpy array of shape (N_test,), containing the target for each testing sample\n",
    "\n",
    "    X_train, y_train, X_test, y_test = None, None, None, None\n",
    "    num_samples = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def prepocess_data(X, y, ratio=0.8):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        ratio: a float number between 0 and 1, representing the ratio of training data\n",
    "    # Output: X_train: a numpy array of shape (N_train, D+1), containing the training data\n",
    "    #         y_train: a numpy array of shape (N_train,), containing the target for each training sample\n",
    "    #         X_test: a numpy array of shape (N_test, D+1), containing the testing data\n",
    "    #         y_test: a numpy array of shape (N_test,), containing the target for each testing sample\n",
    "\n",
    "    X_train, y_train, X_test, y_test = None, None, None, None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "    # Split the data\n",
    "\n",
    "    # Compute the mean and std of the training data\n",
    "\n",
    "    # Normalize the training data\n",
    "   \n",
    "    # Add intercept to both training and testing data\n",
    "   \n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing on the data\n",
    "X_train, y_train, X_test, y_test = prepocess_data(X, y)\n",
    "\n",
    "print('The shape of the training set is:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('The shape of the test set is:')\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction and Loss Implementation\n",
    "\n",
    "Before proceeding with the optimization process, we need to implement two important functions:\n",
    "\n",
    "1. **Prediction Function**: This function will take one or more examples as input and return the corresponding linear predictions.\n",
    "2. **Loss Function**: This function will compute the average loss (or cost) for one or more examples using the cost function from the lab notes.\n",
    "\n",
    "### 2.1 Predict Function\n",
    "\n",
    "The **predict** function will take the weight vector `w` and the input `X` (which could be a single example or multiple examples) and return the predicted output using the linear regression model.\n",
    "\n",
    "### 2.2 Compute Loss Function\n",
    "\n",
    "The **compute_loss** function will compute the **loss** for the given input `X` and targets `y` based on the modelâ€™s predictions. Pne thing that we did not mention in the lab is that for batch gradient descent we will compute \n",
    "\n",
    "#### Steps:\n",
    "- Implement the `predict` function to return linear predictions.\n",
    "- Implement the `compute_loss` function to compute the error for one or more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    # Output: pred: a numpy array of shape (N,), containing the predicted values for the input data\n",
    "\n",
    "    pred = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return pred\n",
    "\n",
    "def compute_loss(X, y, w):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    # Output: loss: a float number representing the average loss\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Gradient Descent Optimization\n",
    "\n",
    "We will optimize the model parameters using gradient descent. We will implement two variants:\n",
    "- **Stochastic Gradient Descent (SGD)**: Updates the model after each training example.\n",
    "- **Batch Gradient Descent**: Updates the model after processing the entire dataset in each iteration.\n",
    "\n",
    "### 3.1 compute_gradient_single\n",
    "\n",
    "This function computes the gradient of the loss function with respect to the weights for a single training example. (vectorize to and compute the gradient vector at once)\n",
    "\n",
    "### 3.2 train_stochastic\n",
    "\n",
    "This function iterates over the training set and updates the weights after each example using SGD. It will return the optimized parameters and a list of all loss values after each update.\n",
    "\n",
    "### 3.3 compute_gradient_batch\n",
    "\n",
    "This function computes the gradient of the loss function with respect to the weights for the entire dataset. You can vectorize such that you can compute the gradient with respect to the entire dataset. Hint: Think what happens when you multiple a transpose of a matrix with another one. (X.T @ ....)\n",
    "\n",
    "### 3.4 train_batch\n",
    "\n",
    "This function implements batch gradient descent. The model is updated after processing the entire dataset in each iteration. Hint: This is already processed in compute_gradient_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_single(x_i, y_i, w):\n",
    "\n",
    "    grad = None \n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return grad\n",
    "\n",
    "def compute_gradient_batch(X, y, w):\n",
    "\n",
    "    grad = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return grad\n",
    "\n",
    "def train_stochastic(X, y, w, alpha, no_iterations):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    #        alpha: a float number representing the learning rate\n",
    "    #        no_iterations: an integer representing the number of iterations\n",
    "    # Output: w: a numpy array of shape (D,), the weights of the trained linear model\n",
    "    #         losses: a list of floats containing the loss at each update\n",
    "    \n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(no_iterations):\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "\n",
    "        #########################################\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "def train_batch(X, y, w, alpha, no_iterations):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    #        alpha: a float number representing the learning rate\n",
    "    #        no_iterations: an integer representing the number of iterations\n",
    "    # Output: w: a numpy array of shape (D,), the weights of the trained linear model\n",
    "    #         losses: a list of floats containing the loss at each update\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for step in range(no_iterations):\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "        #########################################\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "def test(X, y, w):\n",
    "\n",
    "    loss = None\n",
    "    \n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    ######################################### \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's train and test our models\n",
    "\n",
    "w_stochastic = np.random.randn(X_train.shape[1])\n",
    "w_batch = np.random.randn(X_train.shape[1])\n",
    "alpha = 0.00003\n",
    "\n",
    "w_stochastic, losses_stochastic = train_stochastic(X_train, y_train, w_stochastic, alpha, no_iterations=10)\n",
    "\n",
    "w_batch, losses_batch = train_batch(X_train, y_train, w_batch, alpha, no_iterations=20)\n",
    "\n",
    "loss_stochastic = test(X_test, y_test, w_stochastic)\n",
    "loss_batch = test(X_test, y_test, w_batch)\n",
    "\n",
    "print(f'Stochastic Gradient Descent loss on test: {loss_stochastic}')\n",
    "print(f'Batch Gradient Descent loss on test: {loss_batch}')\n",
    "\n",
    "#Plot side by side the losses of the two algorithms should be 2 difffetn figures\n",
    "plt.figure()\n",
    "plt.plot(losses_stochastic)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Stochastic Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses_batch)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Batch Gradient Descent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Loss value explainability. \n",
    "\n",
    "The loss in this setup does not provide much insight into how the model is performing because it is the sum of all the losses on the test set. To better understand the model's performance on average, we should normalize the loss with respect to the number of test examples it was computed on.\n",
    "\n",
    "After normalization, we can observe that the error is more meaningful, and the model appears to be performing well, as it has a relatively small squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each loss in the array losses_stochastic, divide it by the number of samples in the training set\n",
    "losses_stochastic = [loss / X_train.shape[0] for loss in losses_stochastic]\n",
    "#for each loss in the array losses_batch, divide it by the number of samples in the training set\n",
    "losses_batch = [loss / X_train.shape[0] for loss in losses_batch]\n",
    "\n",
    "loss_stochastic = loss_stochastic / X_test.shape[0]\n",
    "loss_batch = loss_stochastic / X_test.shape[0]\n",
    "\n",
    "print(f'Stochastic Gradient Descent loss on test: {loss_stochastic}')\n",
    "print(f'Batch Gradient Descent loss on test: {loss_batch}')\n",
    "\n",
    "#Plot side by side the losses of the two algorithms should be 2 difffetn figures\n",
    "plt.figure()\n",
    "plt.plot(losses_stochastic)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Stochastic Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses_batch)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Batch Gradient Descent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Learning rate parameter tunning\n",
    "\n",
    "Perform the following experiments where we have changed the values of the learning rate. Analyze the results and answer or revise Question 3 from the lab work. Explain the differences between the behavior of each learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run train with different learning rates\n",
    "alpha = [0.00001, 0.00003, 0.0001, 0.0003]\n",
    "\n",
    "losses_viz = []\n",
    "\n",
    "for a in alpha:\n",
    "    w = np.random.randn(X_train.shape[1])\n",
    "    w, loss_i = train_stochastic(X_train, y_train, w, a, no_iterations=10)\n",
    "    losses_viz.append(loss_i)\n",
    "    loss = test(X_test, y_test, w)\n",
    "\n",
    "#plot the losses\n",
    "plt.figure()\n",
    "for i in range(len(alpha)):\n",
    "    plt.plot(losses_viz[i], label=f'alpha={alpha[i]}')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Batch Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Closed Form Solution\n",
    "\n",
    "Lastly, we would like to learn the logistic regression parameters using the closed-form solution. Since we already have the `predict` and `test` methods, we just need to implement the learning method.\n",
    "\n",
    "Your task is to implement the `get_closed_form_solution` function to obtain the desired weights, using the closed-form formula provided in the lab notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closed_form_solution(X_train, y_train):\n",
    "\n",
    "    # Input: X_train: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y_train: a numpy array of shape (N,), containing the target for each sample\n",
    "    # Output: w: a numpy array of shape (D,), the weights of the trained linear model\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return w\n",
    "\n",
    "w_closed_form = get_closed_form_solution(X_train, y_train)\n",
    "y_pred = predict(X_test, w_closed_form)\n",
    "loss = test(X_test, y_test, w_closed_form) / X_test.shape[0]\n",
    "\n",
    "print(f'Closed form solution loss on test: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Weights Analysis\n",
    "\n",
    "One of the advantages of linear models is their interpretability once they are trained. Each weight corresponds to a specific feature, and its value reflects the strength and direction of that feature's influence on the model.\n",
    "\n",
    "Now, examine the values of each weight with respect to the feature it represents. Intuitively, explain why those values make sense in the context of the housing price data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(feature_names, w_closed_form[1:])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Weights')\n",
    "plt.title('Weights of the closed form solution')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml2)",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
