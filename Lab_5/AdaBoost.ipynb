{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('moons_data.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"coolwarm\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Moons data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, criterion=\"gini\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y)\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(set(y))\n",
    "\n",
    "        # regularization - stop if max_depth is reached or if there is only one label\n",
    "        if (depth >= self.max_depth or num_labels == 1):\n",
    "            return {\"label\": Counter(y).most_common(1)[0][0]}\n",
    "        \n",
    "        best_split = self.best_split(X, y, num_features)\n",
    "\n",
    "        if best_split[\"gain\"] == 0:\n",
    "            return {\"label\": Counter(y).most_common(1)[0][0]}\n",
    "\n",
    "        left = self.grow_tree(best_split[\"X_left\"], best_split[\"y_left\"], depth + 1)\n",
    "        right = self.grow_tree(best_split[\"X_right\"], best_split[\"y_right\"], depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature\": best_split[\"feature\"],\n",
    "            \"threshold\": best_split[\"threshold\"],\n",
    "            \"left\": left,\n",
    "            \"right\": right,\n",
    "        }\n",
    "\n",
    "    # Find the best split for the current node in the tree with respect to one feature\n",
    "    # input: X, y, num_features\n",
    "    # output: split -> dictionary containing the feature, threshold, gain, X_left, y_left, X_right, y_right\n",
    "    def best_split(self, X, y, num_features):\n",
    "\n",
    "        best_gain = -1\n",
    "        feature, threshold, gain, X_left, X_right, y_left, y_right = None, None, None , None, None, None, None\n",
    "        split = {\n",
    "            \"feature\": feature,\n",
    "            \"threshold\": threshold,\n",
    "            \"gain\": gain,\n",
    "            \"X_left\": X_left,\n",
    "            \"y_left\": y_left,\n",
    "            \"X_right\": X_right,\n",
    "            \"y_right\": y_right,\n",
    "        }\n",
    "        \n",
    "        ########## Your code goes here ##########\n",
    "        for feature in range(num_features):\n",
    "            X_col = X[:, feature]\n",
    "            thresholds = np.unique(X_col)\n",
    "            for threshold in thresholds:\n",
    "                gain, X_left, y_left, X_right, y_right = self.split(X, y, feature, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split = {\n",
    "                        \"feature\": feature,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"gain\": gain,\n",
    "                        \"X_left\": X_left,\n",
    "                        \"y_left\": y_left,\n",
    "                        \"X_right\": X_right,\n",
    "                        \"y_right\": y_right,\n",
    "                    }\n",
    "        ###########################################\n",
    "\n",
    "        return split\n",
    "\n",
    "    # split the data into two regions based on the feature and threshold and computes the information gain\n",
    "    def split(self, X, y, feature, threshold):\n",
    "        left_idx = X[:, feature] <= threshold\n",
    "        right_idx = X[:, feature] > threshold\n",
    "        X_left, y_left = X[left_idx], y[left_idx]\n",
    "        X_right, y_right = X[right_idx], y[right_idx]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            return 0, X_left, y_left, X_right, y_right\n",
    "\n",
    "        gain = self.information_gain(y, y_left, y_right)\n",
    "\n",
    "        return gain, X_left, y_left, X_right, y_right\n",
    "\n",
    "    # Compute the information gain\n",
    "    # Note that you must weight the left and right child nodes by the number of samples in each\n",
    "    def information_gain(self, y, y_left, y_right):\n",
    "\n",
    "        gain = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "        n_left = len(y_left)\n",
    "        n_right = len(y_right)\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        if self.criterion == \"gini\":\n",
    "            impurity_parent = self.gini(y)\n",
    "            impurity_left = self.gini(y_left)\n",
    "            impurity_right = self.gini(y_right)\n",
    "        else:\n",
    "            impurity_parent = self.entropy(y)\n",
    "            impurity_left = self.entropy(y_left)\n",
    "            impurity_right = self.entropy(y_right)\n",
    "\n",
    "        gain = impurity_parent - (n_left / (n_left + n_right)) * impurity_left - (n_right / (n_left + n_right)) * impurity_right\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "        return gain\n",
    "\n",
    "    # Compute the gini impurity\n",
    "    def gini(self, y):\n",
    "\n",
    "        gini_impurity = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)\n",
    "        gini_impurity = 1 - np.sum(probs ** 2)\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "        return gini_impurity\n",
    "\n",
    "    # Compute the entropy\n",
    "    def entropy(self, y):\n",
    "\n",
    "        entropy_value = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)\n",
    "        entropy_value = -np.sum(probs * np.log2(probs + 1e-9))\n",
    "        ###########################################\n",
    "\n",
    "        return entropy_value\n",
    "    \n",
    "    # Predict for a vector of inputs\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(inputs, self.tree) for inputs in X])\n",
    "\n",
    "    # Predict for a single input\n",
    "    def predict_single(self, inputs, tree):\n",
    "        if \"label\" in tree:\n",
    "            return tree[\"label\"]\n",
    "        feature = tree[\"feature\"]\n",
    "        threshold = tree[\"threshold\"]\n",
    "        if inputs[feature] <= threshold:\n",
    "            return self.predict_single(inputs, tree[\"left\"])\n",
    "        else:\n",
    "            return self.predict_single(inputs, tree[\"right\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []  # To store alpha values\n",
    "        self.models = []  # To store weak classifiers\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        # Initialize weights equally\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        \n",
    "        # Convert y to {+1, -1} for compatibility. Hint: Use np.where\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "\n",
    "            # Implement the Adaboost algorithm using Decision Stumps as weak learners\n",
    "            \n",
    "            ########## Your code goes here ##########\n",
    "\n",
    "            # Create a weak learner\n",
    "            \n",
    "            # Calculate error rate\n",
    "        \n",
    "            # Calculate alpha (importance of the weak learner)\n",
    "            \n",
    "            # Update weights\n",
    "\n",
    "            # Normalize weights\n",
    "\n",
    "            #########################################\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute the weighted sum of weak classifiers\n",
    "\n",
    "        pred = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        return pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost = AdaBoost(n_estimators=35)\n",
    "ada_boost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(tree, X, y, title=\"\"):\n",
    "    # Define bounds of the plot\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    \n",
    "    # Create a grid of points with a small step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    # Use the classifier to predict the class at each grid point\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = tree.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the contours\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"coolwarm\")\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundaries(ada_boost, X, y, title=\"Decision Boundary of the AdaBoost\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
